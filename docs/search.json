[
  {
    "objectID": "00labs.html",
    "href": "00labs.html",
    "title": "Lab Notes",
    "section": "",
    "text": "What are the Labs about?\n\n\n\nLab\nTopics\nOld Labs?\n\n\n\n\nLab 1\nCan you build a data viz with Quarto and submit it to us successfully? Can you compare two regressions fit to a training sample and a validation sample?\n2022 Lab 2 Question 1 is viz, 2022 Lab 2 Question 4 could be adapted to part 2 here\n\n\nLab 2\nCan you use survey weights to complete some sort of inference, can you fit a two-way ANOVA model with interaction?\nANOVA is 2022 Lab 1 Question 2, Survey Weights is from 2022 Quiz 2\n\n\nLab 3\nCan you build a linear regression model using non-linear terms with ols and Spearman \\(\\rho^2\\)?\nThis is basically 2022 Lab 3 Part B\n\n\nLab 4\nCan you build a simple logistic model and use it to make predictions, and then can you build a more complex logistic regression model with non-linear terms, and compare it to a main effects fit?\nSimple model is 2022 Lab 2 Question 5. Complex model is essentially 2022 Lab 3 Part A\n\n\nLab 5\nCan you build a Table 1, and can you use results from a lasso approach to select predictors and evaluate results against a validation sample?\nTable 1 is 2022 Lab 1 Question 1, LASSO is from 2022 Quiz 2\n\n\nLab 6\nCan you run and describe a Kaplan-Meier curve, and can you complete a regression on a count outcome?\nK-M is sort of Lab 6 Q5 (without requiring log rank test) Count is Lab 4 Q4\n\n\nLab 7\nCan you run and describe regression results from an ordinal outcome, and use multiple imputation in a logistic model?\nOrdinal is Lab 4 Q3, MI is from 2022 Quiz 2\n\n\nLab 8\nCan you fit and describe a Cox model with cpm and then can you use tidymodels to assess a linear model\nTidymodels is part of 2022 Lab 5, Cox is part of 2022 Lab 6 questions 2-4"
  },
  {
    "objectID": "calendar.html",
    "href": "calendar.html",
    "title": "Calendar for 432: Spring 2023",
    "section": "",
    "text": "Link\nDate\nDetails\n\n\n\n\nClass 01\nTUE  01-17\nGetting Started, Using Quarto, Validation of Prediction Models\n\n\n–\nWED  01-18\nWelcome to 432 Survey and Campuswire sign-up due at Noon.\n\n\nClass 02\nTHU  01-19\nBuilding a Detailed Project with NHANES Data (reviewing 431 ideas)\n\n\n–\nMON  01-23\nLab 1 due at 9 PM.\n\n\nClass 03\nTUE  01-24\nDeveloping Inferences using Survey Weights\n\n\n–\nWED  01-25\nMinute Paper after Class 03 due at Noon.\n\n\nClass 04\nTHU  01-26\nLinear Regression and ANOVA/ANCOVA models\n\n\n–\nMON  01-30\nLab 2 due at 9 PM.\n\n\nClass 05\nTUE  01-31\nLinear Regression using Non-linear terms and ols"
  },
  {
    "objectID": "calendar.html#february-2023",
    "href": "calendar.html#february-2023",
    "title": "Calendar for 432: Spring 2023",
    "section": "February 2023",
    "text": "February 2023\n\n\n\nLink\nDate\nDetails\n\n\n\n\n–\nWED  02-01\nMinute Paper after Class 05 due at Noon.\n\n\nClass 06\nTHU  02-02\nBuilding Linear Models Effectively, Troubleshooting\n\n\n–\nMON  02-06\nLab 3 due at 9 PM.\n\n\nClass 07\nTUE  02-07\nLogistic Regression for Binary Outcomes\n\n\n–\nWED  02-08\nMinute Paper after Class 07 due at Noon.\n\n\nClass 08\nTHU  02-09\nLogistic Regression with More Predictors\n\n\n–\nMON  02-13\nProject A plan due at 9 PM.\n\n\nClass 09\nTUE  02-14\nLogistic Regression using Non-linear terms and lrm\n\n\n–\nWED  02-15\nMinute Paper after Class 09 due at Noon.\n\n\nClass 10\nTHU  02-16\nBuilding Logistic Models Effectively, Troubleshooting\n\n\n–\nMON  02-20\nLab 4 due at 9 PM.\n\n\nClass 11\nTUE  02-21\nRidge Regression and the Lasso, Variable Selection\n\n\n–\nWED  02-22\nMinute Paper after Class 11 due at Noon.\n\n\nClass 12\nTHU  02-23\nBuilding Table 1  Quiz 1 made available by 5 PM.\n\n\n–\nMON  02-27\nQuiz 1 due at 9 PM.\n\n\nClass 13\nTUE  02-28\nThinking About Power: Retrospective Design"
  },
  {
    "objectID": "calendar.html#march-2023",
    "href": "calendar.html#march-2023",
    "title": "Calendar for 432: Spring 2023",
    "section": "March 2023",
    "text": "March 2023\n\n\n\nLink\nDate\nDetails\n\n\n\n\n–\nWED  03-01\nMinute Paper after Class 13 due at Noon.\n\n\nClass 14\nTHU  03-02\nTime-to-event (survival) data and Kaplan-Meier Curves\n\n\n–\nMON  03-06\nLab 5 due at 9 PM.\n\n\nClass 15\nTUE  03-07\nRegression on Count Outcomes\n\n\n–\nWED  03-08\nMinute Paper after Class 15 due at Noon.\n\n\nClass 16\nTHU  03-09\nRegression on Count Outcomes\n\n\nSpring  Break\nWeek of  03-13\nCWRU Spring Break\n\n\n–\nMON  03-20\nProject A Portfolio due at 9 PM.\n\n\nClass 17\nTUE  03-21\nRegression on Multi-Categorical Outcomes\n\n\n–\nWED  03-22\nMinute Paper after Class 17 due at Noon.\n\n\nClass 18\nTHU  03-23\nPre-recorded class session. Topic TBD.\n\n\n–\nMON  03-27\nLab 6 due at 9 PM.\n\n\nClass 19\nTUE  03-28\nRegression on Multi-Categorical Outcomes\n\n\n–\nWED  03-29\nMinute Paper after Class 19 due at Noon.\n\n\nClass 20\nTHU  03-30\nRegression on Multi-Categorical Outcomes"
  },
  {
    "objectID": "calendar.html#april-2023",
    "href": "calendar.html#april-2023",
    "title": "Calendar for 432: Spring 2023",
    "section": "April 2023",
    "text": "April 2023\n\n\n\nLink\nDate\nDetails\n\n\n\n\n–\nMON  04-03\nLab 7 due at 9 PM.\n\n\nClass 21\nTUE  04-04\nComparing Survival Curves, Fitting Cox Models\n\n\n–\nWED  04-05\nMinute Paper after Class 21 due at Noon.\n\n\nClass 22\nTHU  04-06\nFitting Cox Models with cpm\n\n\n–\nMON  04-10\nProject B Plan due at 9 PM.\n\n\nClass 23\nTUE  04-11\nUsing tidymodels approaches to build linear models\n\n\n–\nWED  04-12\nMinute Paper after Class 23 due at Noon.\n\n\nClass 24\nTHU  04-13\nBuilding Robust Linear Models\n\n\n–\nMON  04-17\nLab 8 due at 9 PM.\n\n\nClass 25\nTUE  04-18\nUsing tidymodels approaches to build logistic models\n\n\n–\nWED  04-19\nMinute Paper after Class 25 due at Noon.\n\n\nClass 26\nTHU  04-20\nTopic TBD. Quiz 2 available by 5 PM.\n\n\n–\nMON  04-24\nQuiz 2 due at 9 PM.\n\n\nClass 27\nTUE  04-25\nTopic TBD.\n\n\n–\nWED  04-26\nMinute Paper after Class 27 due at Noon.\n\n\nClass 28\nTHU  04-27\nFinal Class Session"
  },
  {
    "objectID": "calendar.html#may-2023",
    "href": "calendar.html#may-2023",
    "title": "Calendar for 432: Spring 2023",
    "section": "May 2023",
    "text": "May 2023\n\nFinal Project Presentations (to be scheduled for early May)\nAll course materials, including the Project B Portfolio to be submitted by 12 Noon on 2023-05-09."
  },
  {
    "objectID": "contact.html",
    "href": "contact.html",
    "title": "Contact Us",
    "section": "",
    "text": "Details to come.\nIf you have a question at this time, send it by email to Dr. Love at thomas dot love at case dot edu."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "432 Main Page for Spring 2023",
    "section": "",
    "text": "PQHS/CRSP/MPHP 432 is the second half (431 is the first half) of a two-semester course taught by Professor Thomas Love in the Department of Population and Quantitative Health Sciences at Case Western Reserve University.\nIf you have any questions before class begins, contact Dr. Love at Thomas dot Love at case dot edu."
  },
  {
    "objectID": "index.html#everything-will-appear-here.",
    "href": "index.html#everything-will-appear-here.",
    "title": "432 Main Page for Spring 2023",
    "section": "Everything will appear here.",
    "text": "Everything will appear here.\nEverything that Professor Love will provide to help you with the course will be linked through this website. The menu bar links to just about everything you’ll need this semester, and its elements will go live as they become available. Things you’ll find here include…\n\nThe Course Calendar\nThe Course Syllabus\nProfessor Love’s Course Notes\nDetails on Assignments, including Projects, Quizzes, Labs and Minute Papers\nInformation on installing R and RStudio, installing key R packages, and downloading data, code and templates you’ll need for this class\nAdditional Sources and references, including things to read or watch to supplement our main work\na link to the Canvas system (CWRU log-in required) we use to communicate information about class recordings, and for turning in assignments\na link to the Shared Google Drive\na link to the Campuswire discussion board for the 432 course, and\nmore information on places where you can get help and contact us.\n\nNote that the 431 class materials from Fall 2022 are here, and will remain there until 2023-06-01."
  },
  {
    "objectID": "lab1.html",
    "href": "lab1.html",
    "title": "Lab 1",
    "section": "",
    "text": "Submit your work via Canvas.\nThe deadline for this Lab is specified on the Calendar.\n\nWork submitted late, but within 12 hours of the deadline will lose 5 of the available 50 points.\nWork submitted more than 12 hours after the deadline will lose 10 of the available 50 points.\n\n\nYour response should include a Quarto file (.qmd) and an HTML document that is the result of applying your Quarto file to the data we’ve provided.\n\n\nThere is a Lab 1 Quarto template available on our 432-data page. Please use the template to prepare your response to Lab 1, as it will make things easier for you and for the people grading your work.\n\n\n\nThe oh_counties_2022.csv data set I have provided describes a series of variables, pulled from the data for the 88 counties of the the State of Ohio from the County Health Rankings report for 2022.\n\nThe oh_counties_2022.csv file is available for download on the 432 data page.\nSeveral detailed County Health Rankings files augment these 2022 Ohio Rankings Data. Find those items here if you’re interested. Remember to use the 2022 files.\n\n\n\n\nThe available variables are listed below. Each variable describes data at the COUNTY level.\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nfips\nFederal Information Processing Standard code\n\n\ncounty\nname of County\n\n\nyears_lost_rate\nage-adjusted years of potential life lost rate (per 100,000 population)\n\n\nsroh_fairpoor\n% of adults reporting fair or poor health (via BRFSS)\n\n\nphys_days\nmean number of reported physically unhealthy days per month\n\n\nment_days\nmean number of reported mentally unhealthy days per mo\n\n\nlbw_pct\n% of births with low birth weight (< 2500 grams)\n\n\nsmoker_pct\n% of adults that report currently smoking\n\n\nobese_pct\n% of adults that report body mass index of 30 or higher\n\n\nfood_env\nindicator of access to healthy foods, in points (0 is worst, 10 is best)\n\n\ninactive_pct\n% of adults that report no leisure-time physical activity\n\n\nexer_access\n% of the population with access to places for physical activity\n\n\nexc_drink\n% of adults that report excessive drinking\n\n\nalc_drive\n% of driving deaths with alcohol involvement\n\n\nsti_rate\nChlamydia cases / Population x 100,000\n\n\nteen_births\nTeen births / females ages 15-19 x 1,000\n\n\nuninsured\n% of people under age 65 without insurance\n\n\npcp_ratio\nPopulation to Primary Care Physicians ratio\n\n\nprev_hosp\nRate of hospital stays for ambulatory-care sensitive conditions per 100,000 Medicare enrollees.\n\n\nhsgrads\nHigh School graduation rate\n\n\nunemployed\n% of population age 16+ who are unemployed and looking for work\n\n\npoor_kids\n% of children (under age 18) living in poverty\n\n\nincome_ratio\nRatio of household income at the 80th percentile to income at the 20th percentile\n\n\nassociations\n# of social associations / population x 10,000\n\n\npm2.5\nAverage daily amount of fine particulate matter in micrograms per cubic meter\n\n\nh2oviol\nPresence of a water violation: Yes or No\n\n\nsev_housing\n% of households with at least 1 of 4 housing problems: overcrowding, high housing costs, or lack of kitchen or plumbing facilities\n\n\ndrive_alone\n% of workers who drive alone to work\n\n\nage_adj_mortality\npremature age-adjusted mortality\n\n\ndm_prev\n% of adults with a diabetes diagnosis\n\n\nfreq_phys_distress\n% in frequent physical distress\n\n\nfreq_mental_distress\n% in frequent mental distress\n\n\nfood_insecure\n% who are food insecure\n\n\ninsuff_sleep\n% who get insufficient sleep\n\n\nmedian_income\nestimated median income\n\n\npopulation\npopulation size\n\n\nage65plus\n% of population who are 65 and over\n\n\nafrican_am\n% of population who are African-American\n\n\nhispanic\n% of population who are of Hispanic/Latino ethnicity\n\n\nwhite\n% of population who are White\n\n\nfemale\n% of population who are Female\n\n\nrural\n% of people in the county who live in rural areas\n\n\n\n\n\n\nApplying the clean_names() function from the janitor package as part of the initial oh22 creation process, as I’ve done in my code below, is a sensible strategy. We hope you’ll adopt it when ingesting almost any data you ever try to pull into R.\n\nlibrary(janitor)\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(comment = NA)\n\noh22 <- read_csv(\"https://raw.githubusercontent.com/THOMASELOVE/432-data/master/data/oh_counties_2022.csv\", show_col_types = FALSE) |>\n  clean_names() |>\n  mutate(fips = as.character(fips))\n\noh22\n\n# A tibble: 88 × 43\n   fips  state county    years…¹ sroh_…² phys_…³ ment_…⁴ lbw_pct smoke…⁵ obese…⁶\n   <chr> <chr> <chr>       <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1 39001 Ohio  Adams       11037    25.5    5.41    6.1      9.3    30.2    40  \n 2 39003 Ohio  Allen        8518    20.1    4.53    5.35     9.7    22.8    39.3\n 3 39005 Ohio  Ashland      7769    19.9    4.49    5.44     6      23.6    34.1\n 4 39007 Ohio  Ashtabula    9749    24.7    5.07    5.73     8      27      43  \n 5 39009 Ohio  Athens       7619    22.4    5.02    5.71     8.3    25      34.3\n 6 39011 Ohio  Auglaize     6498    17.3    4.16    5.2      6.7    21.6    39.2\n 7 39013 Ohio  Belmont      8782    20.5    4.48    5.38     8.4    25.8    36.6\n 8 39015 Ohio  Brown       10510    21.2    4.68    5.6      7.7    24.4    37  \n 9 39017 Ohio  Butler       9053    18.7    4.16    5.05     7.8    21.8    34.4\n10 39019 Ohio  Carroll      8066    19.8    4.54    5.5      8.3    24.2    37.1\n# … with 78 more rows, 33 more variables: food_env <dbl>, inactive_pct <dbl>,\n#   exer_access <dbl>, exc_drink <dbl>, alc_drive <dbl>, sti_rate <dbl>,\n#   teen_births <dbl>, uninsured <dbl>, pcp_ratio <dbl>, prev_hosp <dbl>,\n#   hsgrads <dbl>, unemployed <dbl>, poor_kids <dbl>, income_ratio <dbl>,\n#   associations <dbl>, pm2_5 <dbl>, h2oviol <chr>, sev_housing <dbl>,\n#   drive_alone <dbl>, age_adj_mortality <dbl>, dm_prev <dbl>,\n#   freq_phys_distress <dbl>, freq_mental_distress <dbl>, …"
  },
  {
    "objectID": "lab1.html#be-sure-to-include-session-information",
    "href": "lab1.html#be-sure-to-include-session-information",
    "title": "Lab 1",
    "section": "Be sure to include Session Information",
    "text": "Be sure to include Session Information\nPlease display your session information at the end of your submission, as shown below.\n\nxfun::session_info()\n\nR version 4.2.2 (2022-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 22621)\n\nLocale:\n  LC_COLLATE=English_United States.utf8 \n  LC_CTYPE=English_United States.utf8   \n  LC_MONETARY=English_United States.utf8\n  LC_NUMERIC=C                          \n  LC_TIME=English_United States.utf8    \n\nPackage version:\n  askpass_1.1         assertthat_0.2.1    backports_1.4.1    \n  base64enc_0.1.3     bit_4.0.5           bit64_4.0.5        \n  blob_1.2.3          broom_1.0.2         bslib_0.4.2        \n  cachem_1.0.6        callr_3.7.3         cellranger_1.1.0   \n  cli_3.4.1           clipr_0.8.0         colorspace_2.0-3   \n  compiler_4.2.2      cpp11_0.4.3         crayon_1.5.2       \n  curl_4.3.3          data.table_1.14.6   DBI_1.1.3          \n  dbplyr_2.2.1        digest_0.6.31       dplyr_1.0.10       \n  dtplyr_1.2.2        ellipsis_0.3.2      evaluate_0.19      \n  fansi_1.0.3         farver_2.1.1        fastmap_1.1.0      \n  forcats_0.5.2       fs_1.5.2            gargle_1.2.1       \n  generics_0.1.3      ggplot2_3.4.0       glue_1.6.2         \n  googledrive_2.0.0   googlesheets4_1.0.1 graphics_4.2.2     \n  grDevices_4.2.2     grid_4.2.2          gtable_0.3.1       \n  haven_2.5.1         highr_0.9           hms_1.1.2          \n  htmltools_0.5.4     htmlwidgets_1.6.0   httr_1.4.4         \n  ids_1.0.1           isoband_0.2.6       janitor_2.1.0      \n  jquerylib_0.1.4     jsonlite_1.8.4      knitr_1.41         \n  labeling_0.4.2      lattice_0.20.45     lifecycle_1.0.3    \n  lubridate_1.9.0     magrittr_2.0.3      MASS_7.3.58.1      \n  Matrix_1.5.3        memoise_2.0.1       methods_4.2.2      \n  mgcv_1.8.41         mime_0.12           modelr_0.1.10      \n  munsell_0.5.0       nlme_3.1.160        openssl_2.0.5      \n  parallel_4.2.2      pillar_1.8.1        pkgconfig_2.0.3    \n  prettyunits_1.1.1   processx_3.8.0      progress_1.2.2     \n  ps_1.7.2            purrr_0.3.5         R6_2.5.1           \n  rappdirs_0.3.3      RColorBrewer_1.1.3  readr_2.1.3        \n  readxl_1.4.1        rematch_1.0.1       rematch2_2.1.2     \n  reprex_2.0.2        rlang_1.0.6         rmarkdown_2.19     \n  rstudioapi_0.14     rvest_1.0.3         sass_0.4.4         \n  scales_1.2.1        selectr_0.4.2       snakecase_0.11.0   \n  splines_4.2.2       stats_4.2.2         stringi_1.7.8      \n  stringr_1.5.0       sys_3.4.1           tibble_3.1.8       \n  tidyr_1.2.1         tidyselect_1.2.0    tidyverse_1.3.2    \n  timechange_0.1.1    tinytex_0.43        tools_4.2.2        \n  tzdb_0.3.0          utf8_1.2.2          utils_4.2.2        \n  uuid_1.1.0          vctrs_0.5.1         viridisLite_0.4.1  \n  vroom_1.6.0         withr_2.5.0         xfun_0.35          \n  xml2_1.3.3          yaml_2.3.6"
  },
  {
    "objectID": "lab2.html",
    "href": "lab2.html",
    "title": "Lab 2",
    "section": "",
    "text": "Submit your work via Canvas.\nThe deadline for this Lab is specified on the Calendar.\n\nWork submitted late, but within 12 hours of the deadline will lose 5 of the available 50 points.\nWork submitted more than 12 hours after the deadline will lose 10 of the available 50 points.\n\n\nYour response should include a Quarto file (.qmd) and an HTML document that is the result of applying your Quarto file to the data we’ve provided.\n\n\nThere is a Lab 2 Quarto template available on our 432-data page. Please use the template to prepare your response to Lab 2, as it will make things easier for you and for the people grading your work. The template is quite generic, and can also be used for other work, including Labs 3-8."
  },
  {
    "objectID": "lab2.html#data-for-question-1",
    "href": "lab2.html#data-for-question-1",
    "title": "Lab 2",
    "section": "Data for Question 1",
    "text": "Data for Question 1\nDr. Love created these data from NHANES 2017-18 Demographics and Questionnaire data, using the code below.\nSpecifically, he used the DEMO_J (Demographics) and HSQ_J (Current Health Status) files, which are described at this link.\n\nlibrary(nhanesA)\nlibrary(janitor)\nlibrary(tidyverse)\n\ntemp1 <- nhanes('DEMO_J')\ntemp2 <- nhanes('HSQ_J')\ntemp3 <- nhanes('PAQ_J')\n\ntemp12 <- inner_join(temp1, temp2, by = \"SEQN\")\ntemp123 <- inner_join(temp12, temp3, by = \"SEQN\")\n\nlab2q1 <- temp123 |>\n  select(SEQN, WTINT2YR, RIDAGEYR, HSD010, PAQ665) |> \n  filter(RIDAGEYR > 20 & RIDAGEYR < 50) |>\n  filter(PAQ665 < 3) |>\n  mutate(HSD010 = factor(HSD010),\n         PAQ665 = factor(PAQ665),\n         SEQN = as.character(SEQN)) |>\n  clean_names() |>\n  tibble()\n\nrm(temp1, temp12, temp2, temp3, temp123)\n\nsaveRDS(lab2q1, file = \"data/lab2q1.Rds\")"
  },
  {
    "objectID": "lab2.html#variables-studied-in-question-1",
    "href": "lab2.html#variables-studied-in-question-1",
    "title": "Lab 2",
    "section": "Variables Studied in Question 1",
    "text": "Variables Studied in Question 1\nThe resulting variables are listed below.\n\n\n\n\n\n\n\n\nItem\nDescription\nPossible Responses\n\n\n\n\nseqn\nSubject id code\n93717 through 102956\n\n\nwtint2yr\nFull sample 2 year interview weight\nmin = 4363, max = 387879\n\n\nridageyr\nAge in years at screening\nmin = 21, max = 49\n\n\nhsd010\nGeneral Health Condition\nsee below\n\n\npaq665\nModerate Recreational Activities\nsee below\n\n\n\n\nhsd010 Would you say your health in general is\n\n1 = Excellent,\n2 = Very Good,\n3 = Good,\n4 = Fair, or\n5 = Poor?\n(Note that 7 = Refused, 9 = Don’t know in this variable, which we will treat as missing.)\n\npaq665 Do you do any moderate-intensity sports, fitness, or recreational activities that cause a small increase in breathing or heart rate such as brisk walking, bicycling, swimming, or golf for at least 10 minutes continuously?\n\n1 = Yes, 2 = No"
  },
  {
    "objectID": "lab2.html#loading-the-question-1-data",
    "href": "lab2.html#loading-the-question-1-data",
    "title": "Lab 2",
    "section": "Loading the Question 1 Data",
    "text": "Loading the Question 1 Data\nI have provided the saved lab2q1.Rds file to you on the 432-data page. I encourage you to load it using the code below.\n\nlibrary(janitor)\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(comment = NA)\n\nlab2q1 <- read_rds(\"https://raw.githubusercontent.com/THOMASELOVE/432-data/master/data/lab2q1.Rds\")\n\nlab2q1\n\n# A tibble: 2,295 × 5\n   seqn  wtint2yr ridageyr hsd010 paq665\n   <chr>    <dbl>    <dbl> <fct>  <fct> \n 1 93717   53249.       22 2      2     \n 2 93718   20257.       45 3      1     \n 3 93729   11760.       42 4      2     \n 4 93738   59333.       26 3      2     \n 5 93746   27135.       25 2      2     \n 6 93755   30922.       26 2      1     \n 7 93761   18939.       44 3      2     \n 8 93763  103670.       40 3      1     \n 9 93766   16414.       36 4      2     \n10 93774  232377.       41 <NA>   1     \n# … with 2,285 more rows"
  },
  {
    "objectID": "lab2.html#question-2-hints",
    "href": "lab2.html#question-2-hints",
    "title": "Lab 2",
    "section": "Question 2 Hints",
    "text": "Question 2 Hints\n\nOne graph you might use would be one to assess the need for an interaction term, probably via a plot of means.\nAnother graph (or perhaps table) to consider for insight would look at the relationship between insurance and beta-blocker status in these subjects.\nPlease explicitly state in your response that you assume that the missingness you observe in these data are MCAR, and that a complete case analysis is thus appropriate for this Question."
  },
  {
    "objectID": "lab2.html#data-for-question-2-hbp3456-data",
    "href": "lab2.html#data-for-question-2-hbp3456-data",
    "title": "Lab 2",
    "section": "Data for Question 2 (hbp3456 data)",
    "text": "Data for Question 2 (hbp3456 data)\nThe (simulated) data in the hbp3456.csv file describe a total of 3456 people living with hypertension (high blood pressure) diagnoses who receive primary care in one of eight practices.\n\nIn each of the eight practices, 432 (different) individuals (who I’ll call subjects in what follows) were sampled at random from all eligible subjects.\nThe data are based on real electronic health record (EHR) data, but with some noise added.\n\nThe practices are named after streets that appear in The Simpsons.\nThere are 62 (fictional) providers identified across the eight practices, and each provider cares for subjects within a single practice.\n\n\n\nEligibility Criteria\nThe data are cross-sectional and describe results from a one-year reporting window. To be eligible for the study, a subject had to meet all of the following criteria:\n\nhave an EHR-documented hypertension diagnosis which applied during the one-year reporting window,\ncared for at one of the eight practices in this study, and by one of the 62 participating providers in this study\nage 25 or older at the start of the one-year reporting period (note that all subjects with ages 80 and higher are listed as age 80 in the data)\nbetween 1 and 12 primary care office visits in the one-year reporting period\nbetween 2 and 24 primary care office visits combined across the reporting period and the previous year\nfall into one of two biological sex categories (female or male)\nfall into one of four primary insurance categories, specifically Medicare, Commercial, Medicaid or Uninsured.\nhave a most recent systolic BP between 80 and 220 mm Hg and most recent diastolic BP between 40 and 140 mm Hg, where the systolic BP is at least 15 and no more than 130 mm Hg larger than the diastolic BP.\n\n\n\nCodebook\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\nrecord\nunique code for each subject (six digits, first digit is 9, last indicates practice)\n\n\npractice\nprimary care practice, of which there are eight in the data\n\n\nprovider\nprimary care provider (each practice has multiple providers)\n\n\nage\nsubject’s age as of the start of the reporting period\n\n\nrace\nsubject’s race (4 levels: Asian, AA_Black, White, Other)\n\n\neth_hisp\nis subject of Hispanic/Latino ethnicity? Yes or No\n\n\nsex\nsubject’s sex (F or M)\n\n\ninsurance\nsubject’s primary insurance (Medicare, Commercial, Medicaid, Uninsured)\n\n\nincome\nestimated median income of subject’s home neighborhood (via American Community Survey, to nearest $100)\n\n\nhsgrad\nestimated percentage of adults living in the subject’s home neighborhood who have graduated from high school (via American Community Survey, to the nearest tenth of a percent)\n\n\ntobacco\ntobacco use status (Current, Former, or Never)\n\n\ndepr_diag\ndoes subject have depression diagnosis? Yes or No\n\n\nheight\nsubject’s height in meters, rounded to two decimal places\n\n\nweight\nsubject’s weight in kilograms, rounded to one decimal place\n\n\nldl\nsubject’s LDL cholesterol level, in mg/dl\n\n\nstatin\ndoes subject have a current prescription for a statin medication? Yes or No\n\n\nbp_med\ndoes subject have a current prescription for a blood pressure control medication? Yes or No\n\n\nsbp\nsubject’s most recently obtained systolic blood pressure, in mm Hg\n\n\ndbp\nsubject’s most recently obtained diastolic blood pressure, in mm Hg\n\n\nvisits_1\nsubject’s number of visits for primary care in reporting period (one year)\n\n\nvisits_2\nsubject’s visits for primary care in the past two years\n\n\nacearb\ndoes subject have a current prescription for an ACE-inhibitor or ARB? Yes or No\n\n\nbetab\ndoes subject have a current prescription for a beta-blocker? Yes or No\n\n\n\n\n\nNotes on Specific Variables\n\nThe list of medications included in bp_med is: ACE-inhibitor, ARB, Diuretic, Calcium-Channel Blocker, Beta-Blocker, Alpha-1 Blocker, Centrally acting Alpha-2 Agonist, Vasodilator or other antihypertensive agents. A subject with a current prescription for any of these will have a Yes in bp_med.\nFor the acearb, betab, bpmed, statin and depr_diag variables, a No response includes all subjects where there’s no evidence in the EHR of meeting the Yes criterion, so that there are no missing values (a missing value is interpreted there as No.)\nFor the height, weight and ldl results, implausible values were treated as missing in preparing the data for you.\nThe race and eth_hisp values are self-reported, and some subjects refused to answer one or both of the relevant questions.\nThe income and hsgrad values are imputed from the subject’s home address, usually at the census block level, but occasionally at the level of the zip code.\n\nWhen a subject’s home address could not be geocoded, these values are noted as missing.\nGeocoded estimates of income below 6500 are reported as 6500, and estimates above 130000 are reported as 130000.\nFor hsgrad, geocoded estimates below 40 are reported as 40, and estimates above 99.9 are reported as 99.9."
  },
  {
    "objectID": "lab2.html#loading-the-data-for-question-2",
    "href": "lab2.html#loading-the-data-for-question-2",
    "title": "Lab 2",
    "section": "Loading the Data for Question 2",
    "text": "Loading the Data for Question 2\nHere’s the approach I took to load and view the hbp3456 data.\n\nlibrary(janitor)\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(comment = NA)\n\nhbp3456 <- read_csv(\"https://raw.githubusercontent.com/THOMASELOVE/432-data/master/data/hbp3456.csv\", show_col_types = FALSE) |>\n  clean_names() |>\n  mutate(record = as.character(record))\n\nhbp3456\n\n# A tibble: 3,456 × 23\n   record practice provider   age race     eth_hisp sex   insura…¹ income hsgrad\n   <chr>  <chr>    <chr>    <dbl> <chr>    <chr>    <chr> <chr>     <dbl>  <dbl>\n 1 900018 Walnut   W_05        64 <NA>     <NA>     F     Medicare  15600   83  \n 2 900024 King     K_07        74 AA_Black No       F     Medicare  16200   92.8\n 3 900037 Sycamore S_06        60 AA_Black No       F     Commerc…  21400   79  \n 4 900043 Highland H_07        46 White    Yes      F     Medicaid  38300   83.5\n 5 900057 Sycamore S_04        59 AA_Black No       M     Commerc…  23200   78.7\n 6 900062 Elm      E_03        54 AA_Black No       M     Commerc…  48600   85.5\n 7 900076 Plympton P_03        74 White    No       M     Commerc…  64200   92.9\n 8 900082 Elm      E_06        73 White    No       M     Medicare  48600   85.5\n 9 900097 Sycamore S_10        58 AA_Black No       F     Commerc…  29900   86.2\n10 900101 Center   C_01        46 AA_Black No       M     Uninsur…  63600   97.5\n# … with 3,446 more rows, 13 more variables: tobacco <chr>, depr_diag <chr>,\n#   height <dbl>, weight <dbl>, ldl <dbl>, statin <chr>, bp_med <chr>,\n#   sbp <dbl>, dbp <dbl>, visits_1 <dbl>, visits_2 <dbl>, acearb <chr>,\n#   betab <chr>, and abbreviated variable name ¹​insurance"
  },
  {
    "objectID": "lab3.html",
    "href": "lab3.html",
    "title": "Lab 3",
    "section": "",
    "text": "Submit your work via Canvas.\nThe deadline for this Lab is specified on the Calendar.\n\nWork submitted late, but within 12 hours of the deadline will lose 5 of the available 50 points.\nWork submitted more than 12 hours after the deadline will lose 10 of the available 50 points.\n\n\nYour response should include a Quarto file (.qmd) and an HTML document that is the result of applying your Quarto file to the data we’ve provided. While we have not provided a specific template for this Lab, we encourage you to adapt the one provided for Lab 2.\n\n\nThis Lab uses the hbp3456 data developed in Lab 2. See the Lab 2 instructions for details on the data set. Back in Lab 2, we loaded the data with this code.\n\nlibrary(janitor)\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(comment = NA)\n\nhbp3456 <- read_csv(\"https://raw.githubusercontent.com/THOMASELOVE/432-data/master/data/hbp3456.csv\", \n                    show_col_types = FALSE) |>\n  clean_names() |>\n  mutate(record = as.character(record))\n\nHere, we will walk through the process of fitting and evaluating linear regression fits to predict a subject’s estimated (neighborhood) median income (the income variable) on the basis of the following five predictors:\n\nthe subject’s neighborhood high school graduation rate, collected in the hsgrad variable\nthe subject’s race category, from the race variable\nthe subject’s Hispanic/Latinx ethnicity category, as shown in eth_hisp,\nthe subject’s age (in the age variable), and\nthe subject’s current tobacco status, available in the tobacco variable.\n\n\n\n\nStart your work by completing the following tasks to create a tibble that we’ll call hbp_b in the answer sketch:\n\nExclude the 25 subjects in hbp3456 who have missing values of either hsgrad or income.\nRestrict your data to the variables we’ll use in our models (the five predictors listed above, the estimated neighborhood income, and the subject identifying code (the record)).\nEnsure that all character variables (other than record) in your tibble are recognized as factors.\nCreate a new variable called sqrtinc which will serve as your response (outcome) for your regression modeling, within your tibble.\nUse set.seed(432) and slice_sample() to select a random sample of 1000 subjects from the tibble.\n\nYour resulting hbp_b tibble should look like this:\n\n\n\n\nhbp_b\n\n# A tibble: 1,000 × 8\n   record income hsgrad race     eth_hisp   age tobacco sqrtinc\n   <chr>   <int>  <dbl> <fct>    <fct>    <int> <fct>     <dbl>\n 1 903574  34800   94.9 White    No          48 Current    187.\n 2 926837  24700   74.2 AA_Black No          55 Current    157.\n 3 929198  14700   40   AA_Black No          35 Never      121.\n 4 932367  24700   74.2 AA_Black No          41 Never      157.\n 5 925592  65600   92.2 <NA>     <NA>        61 Never      256.\n 6 932404  18500   67.8 AA_Black No          67 Never      136.\n 7 933953  21500   84.4 White    No          72 Never      147.\n 8 911527  23000   83.6 White    No          62 Never      152.\n 9 918228  13400   70.3 AA_Black No          52 Current    116.\n10 930262  48300   90   <NA>     <NA>        73 Never      220.\n# … with 990 more rows"
  },
  {
    "objectID": "lab4.html",
    "href": "lab4.html",
    "title": "Lab 4",
    "section": "",
    "text": "General Instructions\n\nSubmit your work via Canvas.\nThe deadline for this Lab is specified on the Calendar.\n\nWork submitted late, but within 12 hours of the deadline will lose 5 of the available 50 points.\nWork submitted more than 12 hours after the deadline will lose 10 of the available 50 points.\n\n\nYour response should include a Quarto file (.qmd) and an HTML document that is the result of applying your Quarto file to the data we’ve provided. While we have not provided a specific template for this Lab, we encourage you to adapt the one provided for Lab 2.\n\n\nQuestion 1. (10 points)\nThis question uses the oh22 data developed in Lab 1. See the Lab 1 instructions for details on the data set. Back in Lab 1, recall that we loaded the data with this code.\n\nlibrary(janitor)\nlibrary(tidyverse)\n\nknitr::opts_chunk$set(comment = NA)\n\noh22 <- read_csv(\"https://raw.githubusercontent.com/THOMASELOVE/432-data/master/data/oh_counties_2022.csv\", show_col_types = FALSE) |>\n  clean_names() |>\n  mutate(fips = as.character(fips))\n\nUse the oh22 data to create a logistic regression model to predict the presence of a water violation (as contained in h2oviol) on the basis of sev_housing and pm2.5. Use a model with main effects only, and annotate your code with text so that it’s extremely clear what you are doing. Specify and then carefully interpret the estimated odds ratio associated with the sev_housing effect and a 90% confidence interval around that estimate in context using complete English sentences. Be sure to get the direction of the effect right in your modeling and description.\n\n\nQuestion 2. (10 points)\nBegin with the hbp3456 data we developed in Lab 2, but now restricted to the following four practices: Center, Elm, Plympton and Walnut, and to subjects with complete data on the ldl and statin variables. We will use these data for Questions 2-5 in this Lab.\nBuild a logistic regression model to predict whether a subject seen in one of those four practices has a statin prescription based on:\n\nthe subject’s current LDL cholesterol level\nwhich of the four practices they receive care from, along with\nthe subject’s age.\n\nFit two models: one with and one without an interaction term between the practice and the LDL level. Include the age variable in each model using a restricted cubic spline with four knots, but without any interaction with the other predictors. Display the coefficients of your two models.\n\n\nQuestion 3. (10 points)\nFor the “no interaction” model from Question 2, interpret the odds ratio associated with the ldl main effect carefully, specifying a 90% confidence interval and what we can conclude from the results.\n\nTo obtain a 90% confidence interval with a fit using one of the rms fitting functions rather than the default 95% interval, the appropriate code would be summary(modelname, conf.int = 0.9).\nHint: We assume you will describe the ldl main effect by considering the case of Harry and Sally. Harry has an ldl value of 142, equal to the 75th percentile ldl value in the data. Sally has an ldl value of 85, equal to the 25th percentile ldl value in the data. Assume Harry and Sally are the same age and receive care at the same practice. So the odds ratio of interest here compares the odds of statin prescription for Harry to the odds of statin prescription for Sally.\n\n\n\nQuestion 4. (10 points)\nNow using the “interaction” model from Question 2, please interpret the effect of ldl on the odds of a statin prescription appropriately, specifying again what we can conclude from the results. A detailed description of the point estimate(s) will be sufficient here.\n\nHere, we want you to describe the ldl main effect by considering the case of Harry and Sally. Harry has an ldl value of 142, equal to the 75th percentile ldl value in the data. Sally has an ldl value of 85, equal to the 25th percentile ldl value in the data. Assume Harry and Sally are the same age and receive care at the same practice. So the odds ratio of interest here compares the odds of statin prescription for Harry to the odds of statin prescription for Sally. But now, you need to be able to do this separately for each individual level of practice, since practice interacts with ldl. There are at least two ways to accomplish this.\n\nIn one approach, you would create predicted odds values for Harry and Sally, assuming a common age (40 would be a reasonable choice, and it’s the one used in the answer sketch) with ldl set to 142 for Harry and 85 for Sally, but creating four different versions of Harry and Sally (one for each practice.) Then use those predicted odds within each practice to obtain practice-specific odds ratios.\nIn the other approach, you could convince the rms package to use a different practice as the choice for which adjustments are made. By default, datadist chooses the modal practice. To change this, you’d need to convince datadist instead to choose its practice based on which practice is the first one, and relevel the practice factor accordingly. So, if you’d releveled the practice data so that Elm was first and placed that into a tibble called dataelm, you could use the following adjustment to the datadist call to ensure that the adjustments made by datadist used Elm instead of the modal practice.\n\n\n\nd_elm <- datadist(dataelm, adjto.cat = \"first\")\noptions(datadist = \"d_elm\")\n\n\n\nQuestion 5. (10 points)\nNow, compare the effectiveness of your two fitted models (the “interaction” and “no interaction” models) from Question 2 and draw a reasoned conclusion about which of those two models is more effective in describing the available set of observations (after those without statin data are removed) from these four practices. An appropriate response will make use of at least two different validated assessments of fit quality. Be sure to justify your eventual selection (between the “interaction” or “no interaction” model) with complete sentences.\n\nThe natural choices for validated assessments of fit quality in Question 5 are a bootstrap-validated C statistic and a bootstrap-validated Nagelkerke \\(R^2\\). In the answer sketch, we will use 2023 as our random seed for this work, and we’ll use the default amount of bootstrap replications.\n\n\n\nSession Information\nPlease display your session information at the end of your submission, as shown below.\n\nxfun::session_info()\n\nR version 4.2.2 (2022-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 22621)\n\nLocale:\n  LC_COLLATE=English_United States.utf8 \n  LC_CTYPE=English_United States.utf8   \n  LC_MONETARY=English_United States.utf8\n  LC_NUMERIC=C                          \n  LC_TIME=English_United States.utf8    \n\nPackage version:\n  askpass_1.1         assertthat_0.2.1    backports_1.4.1    \n  base64enc_0.1.3     bit_4.0.5           bit64_4.0.5        \n  blob_1.2.3          broom_1.0.2         bslib_0.4.2        \n  cachem_1.0.6        callr_3.7.3         cellranger_1.1.0   \n  cli_3.4.1           clipr_0.8.0         colorspace_2.0-3   \n  compiler_4.2.2      cpp11_0.4.3         crayon_1.5.2       \n  curl_4.3.3          data.table_1.14.6   DBI_1.1.3          \n  dbplyr_2.2.1        digest_0.6.31       dplyr_1.0.10       \n  dtplyr_1.2.2        ellipsis_0.3.2      evaluate_0.19      \n  fansi_1.0.3         farver_2.1.1        fastmap_1.1.0      \n  forcats_0.5.2       fs_1.5.2            gargle_1.2.1       \n  generics_0.1.3      ggplot2_3.4.0       glue_1.6.2         \n  googledrive_2.0.0   googlesheets4_1.0.1 graphics_4.2.2     \n  grDevices_4.2.2     grid_4.2.2          gtable_0.3.1       \n  haven_2.5.1         highr_0.9           hms_1.1.2          \n  htmltools_0.5.4     htmlwidgets_1.6.0   httr_1.4.4         \n  ids_1.0.1           isoband_0.2.6       janitor_2.1.0      \n  jquerylib_0.1.4     jsonlite_1.8.4      knitr_1.41         \n  labeling_0.4.2      lattice_0.20.45     lifecycle_1.0.3    \n  lubridate_1.9.0     magrittr_2.0.3      MASS_7.3.58.1      \n  Matrix_1.5.3        memoise_2.0.1       methods_4.2.2      \n  mgcv_1.8.41         mime_0.12           modelr_0.1.10      \n  munsell_0.5.0       nlme_3.1.160        openssl_2.0.5      \n  parallel_4.2.2      pillar_1.8.1        pkgconfig_2.0.3    \n  prettyunits_1.1.1   processx_3.8.0      progress_1.2.2     \n  ps_1.7.2            purrr_0.3.5         R6_2.5.1           \n  rappdirs_0.3.3      RColorBrewer_1.1.3  readr_2.1.3        \n  readxl_1.4.1        rematch_1.0.1       rematch2_2.1.2     \n  reprex_2.0.2        rlang_1.0.6         rmarkdown_2.19     \n  rstudioapi_0.14     rvest_1.0.3         sass_0.4.4         \n  scales_1.2.1        selectr_0.4.2       snakecase_0.11.0   \n  splines_4.2.2       stats_4.2.2         stringi_1.7.8      \n  stringr_1.5.0       sys_3.4.1           tibble_3.1.8       \n  tidyr_1.2.1         tidyselect_1.2.0    tidyverse_1.3.2    \n  timechange_0.1.1    tinytex_0.43        tools_4.2.2        \n  tzdb_0.3.0          utf8_1.2.2          utils_4.2.2        \n  uuid_1.1.0          vctrs_0.5.1         viridisLite_0.4.1  \n  vroom_1.6.0         withr_2.5.0         xfun_0.35          \n  xml2_1.3.3          yaml_2.3.6"
  },
  {
    "objectID": "lab5.html",
    "href": "lab5.html",
    "title": "Lab 5",
    "section": "",
    "text": "Submit your work via Canvas.\nThe deadline for this Lab is specified on the Calendar.\n\nWork submitted late, but within 12 hours of the deadline will lose 5 of the available 50 points.\nWork submitted more than 12 hours after the deadline will lose 10 of the available 50 points.\n\n\nYour response should include a Quarto file (.qmd) and an HTML document that is the result of applying your Quarto file to the data we’ve provided. While we have not provided a specific template for this Lab, we encourage you to adapt the one provided for Lab 2."
  },
  {
    "objectID": "lab5.html#hints-for-question-1",
    "href": "lab5.html#hints-for-question-1",
    "title": "Lab 5",
    "section": "Hints for Question 1",
    "text": "Hints for Question 1\n\nBe sure that your table specifies the number of subjects in each practice. Note that you’ll have to do something so that your work focuses on the comparison of Highland to Sycamore, leaving out (for this question only) the other practices.\nYou’ll have to deal with some missing values in the data. All missing values are indicated in the .csv file with NA. It’s not usually appropriate to report results that include imputation in a Table 1, so build a note specifying the amount of missing data in a footnote to the table. An appropriate approach would be to produce a list just below your Table. Do not impute for Question 1.\nSome variables will present as characters in the data, but you’d instead prefer them to appear as factors. Be sure to include code in your response to make these changes (the forcats package is your friend here) and then (perhaps using the fct_relevel function in the forcats package) be sure to move the levels of those factors into an order that facilitates interpretation.\nBe sure, too, to make reasoned choices about whether means and standard deviations or instead medians and quartiles are more appropriate displays for the quantitative variables. Include your reasons in a list displayed at the end of your table. Note that the record information is just a code (even though it is numerical) and should be treated as a character variable in using these data, as I did above.\nNote that body mass index (BMI) and BMI category are not supplied in the data, although you do have height and weight. So, you’ll have to calculate the BMI and add it to the data set. If you don’t know the formula for BMI, you have Google to help you figure it out.\nFor BMI categories, use the four groups specified in the How is BMI interpreted for Adults section of this description of Adult BMI by the Centers for Disease Control. Again, you’ll need to use your calculated BMI values and then create the categories in your data set, and you’ll need to figure out a way to accurately get each subject into the correct category.\nDo not include R output without complete sentences describing what you are doing in each step, and what you conclude from that work."
  },
  {
    "objectID": "lab6.html",
    "href": "lab6.html",
    "title": "Lab 6",
    "section": "",
    "text": "Submit your work via Canvas.\nThe deadline for this Lab is specified on the Calendar.\n\nWork submitted late, but within 12 hours of the deadline will lose 5 of the available 50 points.\nWork submitted more than 12 hours after the deadline will lose 10 of the available 50 points.\n\n\nYour response should include a Quarto file (.qmd) and an HTML document that is the result of applying your Quarto file to the data we’ve provided. While we have not provided a specific template for this Lab, we encourage you to adapt the one provided for Lab 2."
  },
  {
    "objectID": "lab6.html#hint-for-question-2",
    "href": "lab6.html#hint-for-question-2",
    "title": "Lab 6",
    "section": "Hint for Question 2",
    "text": "Hint for Question 2\nThe modeling approaches we’ve worked on for count outcomes can be finicky, at least in comparison to OLS. Sometimes, you’ll get to the point where it seems like the model won’t run, or won’t summarize properly, or you have some extremely large or extremely small coefficient estimates or standard errors. Should this happen to you, the first thing we would do is try to identify which of your predictors is causing this problem, by running the model first with one predictor, then two, etc. until you figure out which predictors cause problems. Reasons why you could be having a problem include:\n\na predictor has values that completely identify the category of your outcome variable, perfectly (e.g., one category’s predictor values are inevitably lower than all of another category’s predictor values, with no overlap)\nthe scales of the predictors are wildly different, for instance one predictor has extremely large or extremely small values, causing the estimated standard errors to explode, which should cause you to think about reducing the impact of that, perhaps by changing the units, say from $s to $1000s or by normalizing the predictors\nintense collinearity between two or more of your predictors\ncoding issues in setting up one or more of the variables."
  },
  {
    "objectID": "lab7.html",
    "href": "lab7.html",
    "title": "Lab 7",
    "section": "",
    "text": "Submit your work via Canvas.\nThe deadline for this Lab is specified on the Calendar.\n\nWork submitted late, but within 12 hours of the deadline will lose 5 of the available 50 points.\nWork submitted more than 12 hours after the deadline will lose 10 of the available 50 points.\n\n\nYour response should include a Quarto file (.qmd) and an HTML document that is the result of applying your Quarto file to the data we’ve provided. While we have not provided a specific template for this Lab, we encourage you to adapt the one provided for Lab 2."
  },
  {
    "objectID": "lab7.html#hint-for-question-1",
    "href": "lab7.html#hint-for-question-1",
    "title": "Lab 7",
    "section": "Hint for Question 1",
    "text": "Hint for Question 1\nAll of the issues mentioned in the Hint for Question 2 in Lab 6 apply here, too, perhaps even more so. polr, in particular, can be quite fussy.\nFor example, some people in the past have tried to use median_income in their models along with other variables that have much smaller scales (ranges). we would try rescaling those predictors with large ranges to have similar magnitudes to the other predictors, perhaps simply by expressing the median income in thousands of dollars (by dividing the raw data by 1000) rather than on its original scale, or perhaps by normalizing all of the coefficients by subtracting their means and dividing by their standard deviations.\nAs another example, some people in the past tried using age-adjusted mortality to predict years lost rate, but if you divide the years lost rate into several ordinal categories, it’s not hard to wind up in a situation where age-adjusted mortality is perfectly separated, so that if you know the mortality, it automatically specifies the years lost rate category in these data."
  },
  {
    "objectID": "lab8.html",
    "href": "lab8.html",
    "title": "Lab 8",
    "section": "",
    "text": "Submit your work via Canvas.\nThe deadline for this Lab is specified on the Calendar.\n\nWork submitted late, but within 12 hours of the deadline will lose 5 of the available 50 points.\nWork submitted more than 12 hours after the deadline will lose 10 of the available 50 points.\n\n\nYour response should include a Quarto file (.qmd) and an HTML document that is the result of applying your Quarto file to the data we’ve provided. While we have not provided a specific template for this Lab, we encourage you to adapt the one provided for Lab 2."
  },
  {
    "objectID": "lab8.html#hint-for-question-1",
    "href": "lab8.html#hint-for-question-1",
    "title": "Lab 8",
    "section": "Hint for Question 1",
    "text": "Hint for Question 1\nWhen you build the Spearman \\(\\rho^2\\) plot, use time but not the entire survival object as the “outcome.”"
  },
  {
    "objectID": "notes.html",
    "href": "notes.html",
    "title": "432 Course Notes",
    "section": "",
    "text": "Details to come.\nIf you have a question at this time, send it by email to Dr. Love at thomas dot love at case dot edu."
  },
  {
    "objectID": "packages.html",
    "href": "packages.html",
    "title": "R Packages for 432",
    "section": "",
    "text": "This document contains a list of R packages we will use in 432 and that we expect you to install in your RStudio."
  },
  {
    "objectID": "packages.html#r-packages-to-install",
    "href": "packages.html#r-packages-to-install",
    "title": "R Packages for 432",
    "section": "R Packages to Install",
    "text": "R Packages to Install\nAn R “package” is a collection of functions, data, and documentation that extends the capabilities of R, and is the critical way to get R doing interesting work. These package instructions should be used after you’ve installed R and RStudio. We will add packages to this list as the semester continues.\n\nOpen RStudio. Copy and paste the following lines of code into the Console window of RStudio to install a few key packages.\n\n\npkgs <- c(  \"afex\", \"aplpack\", \"aplore3\", \"arm\", \"babynames\", \"bestglm\", \n            \"boot\", \"broom\", \"car\", \"cowplot\", \"DataExplorer\", \"devtools\", \n            \"Epi\", \"equatiomatic\", \"exact2x2\", \"ez\", \"faraway\", \n            \"fivethirtyeight\", \"foreign\", \"gapminder\", \"gee\", \"geepack\",\n            \"GGally\", \"ggdist\", \"ggforce\", \"ggrepel\", \"ggridges\", \"ggstance\",\n            \"ggthemes\", \"glue\", \"gmodels\", \"gridExtra\", \"gt\", \"gtExtras\", \n            \"gtsummary\", \"here\", \"Hmisc\", \"HSAUR\", \"infer\", \"janitor\", \n            \"kableExtra\", \"knitr\", \"lars\", \"lattice\", \"leaps\", \"lme4\", \n            \"lmerTest\", \"lmtest\", \"lvplot\", \"magrittr\", \"markdown\", \"MASS\", \n            \"mdsr\", \"mice\", \"mitml\", \"modelsummary\", \"moderndive\", \"mosaic\", \n            \"multcomp\", \"naniar\", \"NHANES\", \"nhanesA\", \"nnet\", \"palmerpenguins\",\n            \"party\", \"patchwork\", \"posterdown\", \"pROC\", \"PropCIs\", \"pscl\", \n            \"psych\", \"pwr\", \"qcc\", \"QuantPsyc\", \"quantreg\", \"ResourceSelection\",\n            \"rmarkdown\", \"rmdformats\", \"rms\", \"robustbase\", \"ROCR\", \"rpart\",\n            \"rpart.plot\", \"rstanarm\", \"sandwich\", \"simputation\", \"spelling\",\n            \"styler\", \"summarytools\", \"survival\", \"survminer\", \"tableone\", \n            \"tidymodels\", \"tidyverse\", \"usethis\", \"vcd\", \"VGAM\", \"viridis\", \n            \"visdat\", \"xfun\")\n                \ninstall.packages(pkgs)\n\n\nExecute those commands by hitting Enter.\nNow, go to the Packages tab on the right side of your RStudio screen, and click on Update.\nThis will bring up a dialog box. I usually click Select All, then click Install Updates.\n\n\nA popup box may appear, asking “Do you want to install from sources the packages which need compilation?” to which I usually answer No. A Yes response leads to a slower installation, but can solve problems if you still have them after updating.\nThis may take a few minutes. As long as you’re seeing activity in the Console window, things are progressing.\nEventually, you’ll get a message that “The downloaded source packages are in …” with a directory name. That’s the sign that the updating is done.\nUpdating packages is something you’ll do occasionally throughout the semester, mostly when a problem happens.\n\n\nFinally, choose File … Quit Session from the top menu."
  },
  {
    "objectID": "packages.html#note-a-windows-issue",
    "href": "packages.html#note-a-windows-issue",
    "title": "R Packages for 432",
    "section": "Note: A Windows Issue",
    "text": "Note: A Windows Issue\nIf you are using Windows, and get messages during installation that the latest version of RTools needs to be installed, you can usually just ignore them. If you don’t want to ignore them, go here to download and install RTools for Windows."
  },
  {
    "objectID": "packages.html#the-meta-packages",
    "href": "packages.html#the-meta-packages",
    "title": "R Packages for 432",
    "section": "The Meta-Packages",
    "text": "The Meta-Packages\n\nInstalling the tidyverse meta-package installs the packages listed at https://www.tidyverse.org/.\nInstalling the tidymodels meta-package installs the packages listed at https://www.tidymodels.org/packages/."
  },
  {
    "objectID": "packages.html#a-special-note-on-the-countreg-package",
    "href": "packages.html#a-special-note-on-the-countreg-package",
    "title": "R Packages for 432",
    "section": "A Special Note on the countreg package",
    "text": "A Special Note on the countreg package\nTo build rootograms to visualize the results of regression models on count outcomes, I have decided for the moment to continue to use the countreg package, which is currently available on R-Forge only.\nTo install countreg, type install.packages(\"countreg\", repos=\"http://R-Forge.R-project.org\") into the R Console within R Studio."
  },
  {
    "objectID": "packages.html#installing-a-single-package",
    "href": "packages.html#installing-a-single-package",
    "title": "R Packages for 432",
    "section": "Installing a Single Package",
    "text": "Installing a Single Package\nIf you want to install a single package, you can do so by finding the word Packages on the right side of your RStudio screen.\n\nClick on the Packages tab to start installing the packages you’ll need.\nClick Install, which will bring up a dialog box, where you can type in the names of the packages that you need. These should be separated by a space or comma. Be sure to leave the Install dependencies box checked.\n\n\nA popup box may appear, asking “Do you want to install from sources the packages which need compilation?” to which I usually answer No. A Yes response leads to a slower installation, but can solve problems if you still have them after updating.\nThis may take a few minutes. As long as you’re seeing activity in the Console window, things are progressing.\nEventually, you’ll get a message that “The downloaded source packages are in …” with a directory name. That’s the sign that the updating is done."
  },
  {
    "objectID": "projA.html",
    "href": "projA.html",
    "title": "Project A",
    "section": "",
    "text": "In Project A, you will be analyzing, presenting and discussing a pair of regression models, specifically a linear regression and a logistic regression, describing a data set (available to the public) that you identify.\nThere are two main deliverables:\n\nThe Project A Plan, due in mid-February when the Calendar says it is.\nThe Project A Portfolio and Presentation, due right after Spring Break, as the Calendar indicates.\n\nAll Project A work is to be submitted via Canvas.\n\n\nOn our 432-data page, you will find (in the templates subfolder) a pair of Quarto templates: one for the Project A Plan, and another for the Project A Portfolio. Please use these templates in preparing your work. They will make completing and grading Project A much easier.\n\n\n\nYou can choose either to work alone, or with one other person, to complete Project A. If you work in a group for Project A, you may be asked to work alone for Project B later in the term.\n\nYou will need to identify your Project A partner prior to the submission of your Project A Plan.\nIf you are working with a partner, all work must be submitted by exactly one of you to Canvas while the non-reporting partner submits a one-page note to Canvas indicating the members of the partnership and that the partner will submit the work."
  },
  {
    "objectID": "projA.html#what-makes-a-data-set-acceptable",
    "href": "projA.html#what-makes-a-data-set-acceptable",
    "title": "Project A",
    "section": "What Makes a Data Set Acceptable?",
    "text": "What Makes a Data Set Acceptable?\n\nShared with the World. The data must be available to you, and shared with me and everyone else in the world (without any identifying information) as a well-tidied file by the time you submit your Project A Plan. If the data is from another source, the source (web or other) must be completely identified to me. Ongoing projects that require anyone’s approval to share data are not appropriate for Project A.\n\nYou should have the data in R by 2023-02-01, so that you will have sufficient time to complete the other elements of this Plan. Any data you cannot have by that time is a bad choice.\nFor Project A, you may not use any data set used in the 431 or 432 teaching materials.\nYou must use meaningfully different data sets in 432 Projects A and B.\nIn submitting your Project A Plan, you will need to be able to write “I am certain that it is completely appropriate for these data to be shared with anyone, without any conditions. There are no concerns about privacy or security.” So be sure that’s true before you pick a data set.\n\nSize.\n\nA minimum of 100 complete observations are required on each variable. It is fine if there are some missing values, as well, so long as there are at least 100 rows with complete observations on all variables you intend to use in each model.\nThe maximum data set size is 1200 observations, so if you have something larger than that, you’ll need to select a random subset of the available information as part of your data tidying process.\n\nOutcomes. The columns must include one quantitative outcome and one binary categorical outcome.\n\nWe prefer distinct outcomes, but if necessary, the binary outcome can be generated from the quantitative outcome (as an example, your quantitative outcome could be resting heart rate in beats per minute, and your binary outcome could be whether the resting heart rate is below 70 beats per minute.)\n\nInputs. You will need at least four regression inputs (predictors) for each of your two models.\n\nAt least one of the four must be quantitative (a variable is not quantitative for this purpose unless it has at least 10 different, ordered, observed values), and at least one must be multi-categorical (with between 3 and 6 categories, each containing a minimum of 30 subjects) for each model.\nYour other inputs can represent binary, multi-categorical or quantitative data.\nYou can examine different candidate predictors for each outcome, or use the same ones in both your linear and logistic regression models.\nDepending on your sample size, you can study more than the minimum number of regression inputs. See specifications below for your linear and logistic models.\n\nNo hierarchical data. In Project A, we require you to study cross-sectional data, where rows indicate subjects and columns indicate variables gathered to describe those subjects at a particular moment in time or space. Do not use “nested” or “longitudinal” data in Project A.\n\nThe singular exception to this rule is that it will usually be acceptable for all inputs to be collected at a single (baseline) time point and both outcomes to be collected at a single future point in time. For example, you could predict systolic blood pressure in 2022 (or whether or not a subject’s systolic blood pressure in 2022 was below 140), based on a set of input variables (likely including systolic blood pressure) all gathered in 2021."
  },
  {
    "objectID": "projA.html#good-data-sets-to-use",
    "href": "projA.html#good-data-sets-to-use",
    "title": "Project A",
    "section": "Good Data Sets To Use",
    "text": "Good Data Sets To Use\nSome sources of data we’d like to see people use include:\n\nCDC WONDER data, which could (at the county level) be combined with data from County Health Rankings 2022 to do something interesting.\nThe data sets described in the American Statistical Association’s Data Challenge Expo for 2022, which include five very interesting data sets selected from the Urban Institute Data Catalog\nA data set from the Tidy Tuesday archive or from the Data is Plural archive might be a good candidate.\nThe Health and Retirement Study\nThe General Social Survey although the problem there is a lack of quantitative variables.\nThe many many public use data sets available at ICSPR\nThe 500 Cities and PLACES data portal, most probably focusing on the County-level data and potentially combining it with County Health Rankings 2022.\nNational Center on Health Statistics which includes NHANES (not a good choice for this project) but also other data sets.\nBehavioral Risk Factor Surveillance System\n\n\nWhile data on COVID-19 would be permitted for 432 projects, most of the available data is longitudinal and thus unsuitable for Project A.\nWe are not interested in people using NHANES data in Project A, or County Health Rankings data, unless (as indicated above) those County Health Rankings are combined with meaningful additional data sets.\nKaggle Public Datasets may be permitted, but we discourage this. We will only accept those with really useful variables, no hierarchical structure and strong descriptions of how the data were gathered (which is at best 5% of what’s available on Kaggle). Don’t select a Kaggle data set without running it by us on Campuswire (see below) to see if we’re willing to let you use it.\nDon’t type “regression data sets” into Google. That will lead you to data sets we’ve seen before, and don’t want to see again. The data sets posted by the Cleveland Clinic for educational purposes are a poor choice for Project A, since we’ve seen them before."
  },
  {
    "objectID": "projA.html#running-a-data-set-past-us-for-project-a",
    "href": "projA.html#running-a-data-set-past-us-for-project-a",
    "title": "Project A",
    "section": "Running a Data Set Past Us for Project A",
    "text": "Running a Data Set Past Us for Project A\nTo get Professor Love and the TAs to “sign off” on a data set as appropriate for your Project A Plan, you need to tell us the following four things in a (private or public - your choice) note on Campuswire in the projectA folder. Please do this if you’re not sure your data set is appropriate.\n\nthe data source, as described here, along with a URL so we can access the data\na description of who the subjects are and how they were selected, as described here - it helps if you also tell us how many subjects are in the data.\nwhat quantitative outcome you plan to use in your linear regression model, including its units of measurement and the minimum, mean and maximum values available in your data\nwhat binary outcome you plan to use in your logistic regression model, specifying both of the mutually exclusive and collectively exhaustive categories and how many subjects fall into each of those two categories.\n\nAlso, we ask that you not ask us to pick between two “options” - submit the one you’d rather do. If it’s a problem, we’ll let you know, and you can then change to another option if necessary."
  },
  {
    "objectID": "projA.html#project-a-plan-contents",
    "href": "projA.html#project-a-plan-contents",
    "title": "Project A",
    "section": "Project A Plan Contents",
    "text": "Project A Plan Contents\nPlease use the Quarto template we provided for the Project A Plan in preparing your work.\n\nTitle and Authors\nYour project should have a meaningful title (not containing the words “432” or “Project” or “Proposal” or “Plan”) but rather something describing your actual data and plans.\nPlease keep the title to no more than 80 characters, including spaces. You can add a subtitle if you like, but the main title should stand on its own.\n\n\nR Packages and Setup\nYou’ll load necessary packages at the start in an unnumbered section of your work, following the template.\n\nDo not use source(\"Love-boost.R\") or any other R script or package unless you actually need something it provides.\nDo not load core elements of tidyverse or tidymodels separately. instead just load the meta-packages, and do so last.\nUse #| message: false as part of your Quarto code chunk where the packages are listed (as we have done in the template) to eliminate HTML messages about when packages were built or how objects were masked.\n\n\n\n1. Data Source\nProvide complete information on the source of the data: how did you get it, how was it gathered, by whom, in what setting, for what purpose, and using what sampling strategy.\nThis small section should include a clear link (with all necessary details) to the URL which we can use to obtain the raw data freely.\n\n\n2. The Subjects\nA description (one or two sentences should be sufficient) of who or what the subjects (rows) are in your data set, and how they were sampled from the population of interest.\n\n\n3. Loading and Tidying the Data\n\n3.1 Loading the Raw Data\nProvide code to ingest the raw data. Ideally, this should use tidyverse-friendly code and a direct link to the URL where the data are housed online.\n\n\n3.2 Cleaning the Data (involves several subsections)\nTidy and clean up the data to meet all necessary requirements for your modeling work. This will require multiple sub-sections as you tackle different tasks for different sets of variables. Use the tidyverse for data management whenever possible. Some of things you need to do here…\n\nEliminate all variables that are not going to be used (either as identifiers, outcomes or inputs) in your planned analyses.\nChange variable names to more meaningful ones, although it’s helpful to keep them at 10 characters or less. Use clean_names() from the janitor package to clean up and standardize the presentation of variable names.\nSample the data as needed to ensure that you meet the data set size specifications (no more than 1200 rows, for instance.)\nConvert all variables to appropriate types (factors, etc.) as needed, and complete appropriate checks of the values for all variables.\nIf you have prospective inputs (predictors) that are multi-categorical with more than 6 categories, collapse them to six or fewer categories in a rational way at this stage.\nEnsure that all categorical variables have at least 30 observations at each level, collapsing or removing levels as needed to accomplish this end.\nYour tidied data set should be arranged with a row (subject) identifier at the far left. That identifier should have a unique value for each row, and should be saved as a character variable in R.\nWe expect your final tibble to have some missing values. Do not impute or delete these, but do be sure they are correctly identified as missing values by R.\nDo not list the entire tibble or print out large strands of R output (like summaries of the entire tibble) anywhere in your document, except where you are required by these instructions to do so.\n\n\n\n\n4. The Tidy Tibble\n\n4.1 Listing the Tibble\nIn this section, you should start by listing the tibble you created in Section 3, with all variables correctly imported (via your code) as the types of variables (factor/integer/numeric, etc.) that you need for modeling.\n\nThis should be a listing, not a glimpse or anything else. Just type in the name of your tibble.\nThe resulting list should be limited to the first 10 rows of your data.\n\n\n\n4.2 Size and Identifiers\nWrite a sentence specifying the number of rows and the number of columns in your tibble, and this should match the R output.\nThen identify the name of the variable that provides the identifier for each row in your tidy tibble, and demonstrate that each row has a unique value for that variable, and that the variable is represented as a character in R.\n\nOne way to do this is to run the n_distinct function from the dplyr package on this particular variable.\nDo not present summary or descriptive results on every variable in the whole tibble here - you’ll do that in Section 5.\n\n\n\n4.3 Save the tidied Tibble as an .Rds file\nNow, save the tidied data set as an .Rds file (using write_rds or the equivalent), which you’ll need to submit to us. The tibble should have the same name as the data file you submit to Canvas.\n\n\n\n5. The Code Book\n\n5.1 Defining the Variables\nIn this section, provide a beautiful codebook which tells us (at a minimum) the following information for each variable in the tibble you printed and saved in Section 4.\n\nThe name of the variable in your tibble.\nThe role of the variable in your planned analyses (options include identifier, outcome, or input)\nThe type of variable for each outcome or input (options are categorical, in which case tell us how many categories, or quantitative)\nA short description of the meaning of the variable.\n\nThis should include the units of measurement if the variable is quantitative, and a list of the possible values if the variable is categorical.\n\n\nAs an example, here’s a part of a simple table:\n\n\n\n\n\n\n\n\n\nVariable\nRole\nType\nDescription\n\n\n\n\nsubjectID\nidentifier\n-\ncharacter code for subjects\n\n\nsysbp\noutcome\nquant\nMost Recent Systolic Blood Pressure, in mm Hg\n\n\nstatin\ninput\n2-cat\nHas a current statin prescription? (Yes or No)\n\n\n\n\n\n5.2 Numerical Description\nHere, run the describe command from the Hmisc package on your entire tibble. Be sure that the results match up with what you’ve described in defining the variables, and that the same variables appear, in the same order, in the codebook and in these results.\n\n\n\n6. Linear Regression Plans\n\n6.1 My First Research Question\nBegin this section by specifying a question that you hope to answer with the linear model you are proposing. A research question relates clearly to the data you have and your modeling plans, and, like all questions, it ends with a question mark. Eventually, you will need to answer this question in your portfolio.\nJeff Leek in his book “How to be a Modern Scientist” has some excellent advice in his section on “How Do You Start a Paper.” In particular, you want to identify research questions that:\n\nare concrete, (and for which you can find useful data), and that\nsolve a real problem, and that\ngive you an opportunity to do something new,\nthat you will feel ownership of, and\nthat you want to work on.\n\nWe recommend you use the FINER criteria (or, if relevant, the PICOT criteria) to help you refine a vague notion of a research question into something appropriate for this project.\n\nFINER = Feasible, Interesting, Novel, Ethical and Relevant.\nPICOT is often used in medical studies assessing or evaluating patients and includes Patient (or Problem), Intervention (or Indicator), Comparison group, Outcomes and Time.\n\nThe Wikipedia entry on research questions provides some helpful links to these ideas.\n\n\n6.2 My Quantitative Outcome\nThis subsection tells us what you will use your linear regression model to explain or predict.\n\nTell us the name in the tibble of the linear regression outcome you will use (this should the quantitative outcome you identified in your Codebook) and state why you are interested in predicting this variable.\nProvide a count of the number of rows in your data with complete information on this outcome.\nProvide a nicely labeled graphical summary of the distribution of your outcome to supplement the numerical description you provided in Section 5.2.\nComment briefly on the characteristics of the outcome’s distribution. Is your outcome skewed or symmetric, is it discrete or fairly continuous, is there a natural transformation to consider?\nDemonstrate that the variable you have selected meets the standard for a quantitative variable used in this Project, specifically that it has at least 10 different, ordered, observed values.\n\n\n\n6.3 My Planned Predictors (Linear Model)\nNow, tell us precisely which four (or more) candidate predictors (inputs) you intend to use for your linear regression model.\n\nPlease use the variable names that appear in your code book and tibble.\nDemonstrate to us that you have at least one input which is quantitative, specifically that it has at least 10 different, ordered, observed values.\nDemonstrate to us that you have at least one categorical input which has between 3 and 6 categories, that will be used as a factor in your modeling, and that has at least 30 observations in each level of the factor. If necessary, you can create such a predictor from a quantitative one, but if you are doing this, remember that only the multi-categorical version of the predictor should be included in your models.\nDemonstrate that the total number of candidate predictors you suggest is no more than \\(4 + (N_1 - 100)/100\\), rounding down, where \\(N_1\\) is the number of rows with complete outcome data in your tibble.\n\n\n\n\n7 Logistic Regression Plans\n\n7.1 My Second Research Question\nBegin this section by specifying a question that you hope to answer with the logistic model you are proposing. A research question relates clearly to the data you have and your modeling plans, and, like all questions, it ends with a question mark. Eventually, you will need to answer this question in your portfolio.\nSee section 6.1 for some more suggestions about improving your research questions.\n\n\n7.2 My Binary Outcome\nThis subsection tells us what you will use your logistic regression model to explain or predict.\n\nTell us the name in the tibble of the logistic regression outcome you will use (this should be the binary (2-category) outcome you identified in your codebook) and state why you are interested in predicting this variable.\nProvide a count of the number of rows in your data with each of the two possible values of this outcome. Indicate whether you have any missing values.\n\n\n\n7.3 My Planned Predictors (Logistic Model)\nNow, tell us precisely which four (or more) candidate predictors you intend to use for your logistic regression model.\n\nIf you are using some of the same predictors as in your linear regression model, there’s no need to repeat yourself. Simply tell us which variables you’ll use again, and then provide descriptions for any new predictors that did not appear in your plans for the linear model.\nDemonstrate that the total number of candidate predictors you suggest for your logistic regression model is no more than \\(4 + (N_2 - 100)/100\\) predictors, rounded down, where \\(N_2\\) is the number of subjects in the smaller of your two outcome groups.\n\n\n\n\n8 Affirmation\nNext you need to affirm that the data set meets all of the project requirements, especially that the data can be shared freely over the internet, and that there is no protected information of any kind involved.\nThe text we want to see here is\n\nI am certain that it is completely appropriate for these data to be shared with anyone, without any conditions. There are no concerns about privacy or security.\n\nIf you are unsure as to whether this is true, select a different data set.\n\n\n9 References\nReferences (you’ll need one to describe the source of your data, at least) go here.\n\n10 Session Information\nPlease provide session information by running xfun::session_info().\n\nxfun::session_info()\n\nR version 4.2.2 (2022-10-31 ucrt)\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\nRunning under: Windows 10 x64 (build 22621)\n\nLocale:\n  LC_COLLATE=English_United States.utf8 \n  LC_CTYPE=English_United States.utf8   \n  LC_MONETARY=English_United States.utf8\n  LC_NUMERIC=C                          \n  LC_TIME=English_United States.utf8    \n\nPackage version:\n  base64enc_0.1.3   bslib_0.4.2       cachem_1.0.6      cli_3.4.1        \n  compiler_4.2.2    digest_0.6.31     ellipsis_0.3.2    evaluate_0.19    \n  fastmap_1.1.0     fs_1.5.2          glue_1.6.2        graphics_4.2.2   \n  grDevices_4.2.2   highr_0.9         htmltools_0.5.4   htmlwidgets_1.6.0\n  jquerylib_0.1.4   jsonlite_1.8.4    knitr_1.41        lifecycle_1.0.3  \n  magrittr_2.0.3    memoise_2.0.1     methods_4.2.2     mime_0.12        \n  R6_2.5.1          rappdirs_0.3.3    rlang_1.0.6       rmarkdown_2.19   \n  rstudioapi_0.14   sass_0.4.4        stats_4.2.2       stringi_1.7.8    \n  stringr_1.5.0     tinytex_0.43      tools_4.2.2       utils_4.2.2      \n  vctrs_0.5.1       xfun_0.35         yaml_2.3.6"
  },
  {
    "objectID": "projA.html#your-audience",
    "href": "projA.html#your-audience",
    "title": "Project A",
    "section": "Your Audience",
    "text": "Your Audience\nYour audience for this presentation includes Professor Love, the TAs and your fellow students. Prepare your presentation with that audience in mind. What will they need to know to understand what you’ve done, and get excited about it?"
  },
  {
    "objectID": "projA.html#outline-of-the-presentation",
    "href": "projA.html#outline-of-the-presentation",
    "title": "Project A",
    "section": "Outline of the Presentation",
    "text": "Outline of the Presentation\nYour presentation should include fewer than 10 slides.\nYour “most important finding” is just going to be one of many potentially interesting findings in your Project. Your job in the presentation is not to prove to me that you did a lot of work - I’ll see that in the portfolio.\nInstead, your job in the presentation is to interest your audience in something you found that is (at least relatively) important. You are expected to help us understand the following things related to your most important finding, based on either a linear or logistic model.\nYou will not be developing any new material for the slides (just restating and rearranging things you’ve already done and perhaps constructing a short narrative to help us retain your key findings) once you have the portfolio. As a result, we encourage you to complete the portfolio first.\nWe suggest you develop about 8 slides. This should include…\n\nA title slide\nA couple of slides to describe the Subjects, Outcome and Predictors\n\nMake sure we understand who the subjects are, how they were selected, what the outcome is and why we should care about it, and what predictors are involved in the model you’ll show.\n\nSeveral slides showing meaningful statistical findings (What should we learn from your model?)\n\nWhat does the model (don’t show us details of multiple models in the presentation) say about the relationship between the outcome and the predictors?\nYou’re only showing us one model (of models A, B, Y and Z) in the presentation.\nHow well does this model fit the data you have, and how well might it fit in new data?\n\nA couple of slides discussing next steps\n\nIt is unlikely that you’ll have a model that is truly satisfactory all on its own, so what could be done to improve it that you cannot already do with the data you have? What other data could be collected, how could the measures be refined, could you design a study to get to a more convincing answer?\n\n\nMake sure that you introduce yourself when you start to speak, over your title slide if you are working alone. We’re happy to see your face during the presentation, but this isn’t mandatory. If you are working with a partner, each of you should introduce yourself at the beginning, and let me know who’s speaking first.\n(Essentially) every word and every image/table/chart in your slides can and should come directly from the materials contained in your HTML portfolio.\n\nThe development of the presentation is mostly about selecting useful information to present and then arranging it in a way that sticks for your audience.\nYour presentation should include no R code but instead will provide nicely formatted figures and tables along with text. Some figures don’t work well on slides, like nomograms, without a lot of work. Pick something that is both useful and easy to see.\nEach slide should have a title, indicating the message you want us to get from the slide (don’t use generic titles like “Results” or “Table 1”).\nYou’ll have to cut out around 95% of your portfolio to create your slides, and you should follow your instincts regarding your audience (Professor Love, the TAs and your fellow students are your audience.)\nDeveloping the presentation is where you have to make decisions about what’s most important to show an audience to get them interested in your work. That’s a critically important skill."
  },
  {
    "objectID": "projB.html",
    "href": "projB.html",
    "title": "Project B",
    "section": "",
    "text": "Details to come.\nIf you have a question at this time, send it by email to Dr. Love at thomas dot love at case dot edu."
  },
  {
    "objectID": "quiz1.html",
    "href": "quiz1.html",
    "title": "Quiz 1",
    "section": "",
    "text": "Quiz 1 materials will become available in late February according to the Calendar.\nThe Quiz will include materials from Classes 1-11 of the course.\nIf you have a question prior to the release of Quiz 1 about the Quiz, send it by email to Dr. Love at thomas dot love at case dot edu."
  },
  {
    "objectID": "quiz2.html",
    "href": "quiz2.html",
    "title": "Quiz 2",
    "section": "",
    "text": "Quiz 2 materials will become available in late April according to the Calendar.\nThe Quiz will include materials from Classes 1-25 of the course.\nIf you have a question prior to the release of Quiz 2 about the Quiz, send it by email to Dr. Love at thomas dot love at case dot edu."
  },
  {
    "objectID": "syllabus.html",
    "href": "syllabus.html",
    "title": "432 Syllabus",
    "section": "",
    "text": "Details to come.\nIf you have a question at this time, send it by email to Dr. Love at thomas dot love at case dot edu."
  }
]